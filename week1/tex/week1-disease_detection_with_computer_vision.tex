\documentclass[a4paper,12pt]{article}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}  
\makeatletter
\def\@seccntformat#1{%
	\expandafter\ifx\csname c@#1\endcsname\c@section\else
	\csname the#1\endcsname\quad
	\fi}
\makeatother


\title{Disease Detection with Computer Vision}
\author{By V\'itor Andrade}


\begin{document}
\maketitle

\section{Building and training a model for medical diagnosis}


Labels: Positive, Negative.\\\\
The algorithm produces an output in the form of scores, which are probabilities for each label.\\
Before training these scores are not going to match the desired label.\\\\
\begin{tabular}{|c c c|}
\hline
Desired Label & Score & Error (Loss) \\
\hline
\textbf{Positive: 1} & 0.48 & 0.32 \\
\textbf{Negative: 0} & 0.51 & 0.31 \\
\hline
\end{tabular}\\\\
After training:\\
\begin{tabular}{|c c c|}
	\hline
	Desired Label & Score & Error (Loss) \\
	\hline
	\textbf{Positive: 1} & 0.60 & 0.22 \\
	\textbf{Negative: 0} & 0.30 & 0.15 \\
	\hline
\end{tabular}\\\\
\\
We can measure this error by \textbf{computing a loss function}. A loss function measures the error between our output probability and the desired label.
\begin{equation}
Probabilities\left\{
\begin{array}{@{}ll@{}}
P(Y=1|X) & \text{if}\ \ y=1 \nonumber \\
P(Y=0|X) & \text{if}\ \ y=0
\end{array}\right.
\end{equation} 
\\\\\\
\section{Three Key challenges}
\begin{itemize}
	\item Class Imbalance
	\item Multi-Task
	\item Dataset Size
\end{itemize}

\subsection{Class Imbalance}
In medical datasets, the number of disease and non-disease examples are not equal. This a reflection of the prevalence or frequency of disease in the real-world. In a medical dataset, you might see 100 times as many normal examples as mass examples.\\
This yields a model that starts to predict a very low probability of disease for everybody and won't be able to identify when an example has a disease.\\
\\
Imagine an example of a chest x-ray that contains a mass, so it gets labeled with 1 and the algorithm outputs a probability of 0.2. Now, the 0.2 here is the probability according to the algorithm of Y being equal to 1, the probability that this example is a mass. So now, we can apply the loss function to compute the loss on this example.
\\\\
\textbf{Binary cross-entropy loss:}
\begin{equation}
L(X,y)=\left\{
\begin{array}{@{}ll@{}}
-log P(Y=1|X) & \text{if}\ \ y=1 \nonumber \\
-log P(Y=0|X) & \text{if}\ \ y=0
\end{array}\right.
\end{equation} 

Example Label 1:\\
Probability of Y = 1: P(Y=1|X)\\
L=-log()0.2)=0.10\\

Example Label 0:\\
Probability of Y = 0: P(Y=0|X) = 1 - P(Y=1|X)\\
L=-log(1-0.7)=0.52
\\
\\
The \textbf{solution to the class imbalance problem} is to modify the loss function, to weight the normal and the mass classes differently. 
\\\\
\begin{equation}
L(X,y)=\left\{
\begin{array}{@{}ll@{}}
w_p * -log P(Y=1|X) & \text{if}\ \ y=1 \nonumber \\
w_n * -log P(Y=0|X) & \text{if}\ \ y=0
\end{array}\right.
\end{equation}
\\
\[ w_p=\frac{num\ negative}{num total} \ \ \textrm{ }
\ \ w_n=\frac{num\ positive}{num total} \]
\\
We want to weight the positive examples more, such that they can have an equal contribution overall to the loss, as the normal examples.\\
\\
Another solution to tackle the class imbalance problem is \textbf{re-sampling}. The basic idea here is to re-sample the dataset such that we have an equal number of negative and positive examples.\\
\\
We do this excluding negative examples and duplicating positive examples.

\subsection{Multi-Task}
In the real work, we care about classifying the presence or absence of many diseases. One approach is to have models that each learn one of these tasks.\\
However, maybe we can learn to do all of the tasks using one model. An advantage of this is that we can learn features that are common to identifying more than one disease, allowing us to use our existing data more efficiently.\\
\\
Instead of examples having one label, they now have one label for every disease \textit{(P1 0,1,0)}\\
\\
\begin{tabular}{|c c|}
	\hline
	(mass, pneumonia, edema) & Prediction Probabilities \\
	\hline
	P1 0, 1, 0 & 0.3, 0.1, 0.8 \\
	P2 0, 0, 1 & 0.1, 0.1, 0.8 \\
	P3 0, 1, 1 & 0.2, 0.2, 0.7 \\
	P4 1, 0, 1 & 0.6, 0.3, 0.8 \\
	P5 1, 1, 1 & 0.7, 0.7, 0.9 \\
	P6 1, 0, 0 & 0.8, 0.1, 0.2 \\
	P7 0, 1, 1 & 0.3, 0.9, 0.8 \\
	\hline
\end{tabular}\\
\\
To train such an algorithm, we also need to make the modification to the loss function from the binary tasks to the multitask setting.\\
We can represent our new loss as the \textbf{sum of the losses over the multiple diseases}.\\
\[ L(X,y)=L(X,y_{mass}) + L(X,y_{pneumonia}) + L(X,y_{edema}) \]\\
This is called the \textbf{Multi-Label Loss} or the \textbf{Multi-Task Loss}.\\
\\
Here is the table updated with the new loss.\\
\\
\begin{tabular}{|c c c|}
	\hline
	(mass, pneumonia, edema) & Prediction Probabilities & Loss\\
	\hline
	P1 0, 1, 0 & 0.3, 0.1, 0.8 & 0.52 + 1.00 + 0.70\\
	P2 0, 0, 1 & 0.1, 0.1, 0.8 & 0.05 + 0.05 + 0.10\\
	P3 0, 1, 1 & 0.2, 0.2, 0.7 & 0.10 + 0.70 + 0.15\\
	P4 1, 0, 1 & 0.6, 0.3, 0.8 & 0.22 + 0.52 + 0.10\\
	P5 1, 1, 1 & 0.7, 0.7, 0.9 & 0.15 + 0.15 + 0.05\\
	P6 1, 0, 0 & 0.8, 0.1, 0.2 & 0.10 + 0.05 + 0.10\\
	P7 0, 1, 1 & 0.3, 0.9, 0.8 & 0.52 + 0.05 + 0.10\\
	\hline
\end{tabular}\\
\\\\
\textbf{Multi-task + Class imbalance}\\

One final consideration is how we can account for class imbalance in the multitask setting.\\
Once again, we can apply the weighted loss that we have covered earlier. This time, we not only have a weight associated with just the positive and the negative labels, but it's for the positive and negative labels associated with that particular class.\\
\[ L(X,y_{mass}) + L(X,y_{pneumonia}) + L(X,y_{edema}) \]
\begin{equation}
L(X,y_{mass})=\left\{
\begin{array}{@{}ll@{}}
w_{p,mass} * -log P(Y=1|X) & \text{if}\ \ y=1 \nonumber \\
w_{n,mass} * -log P(Y=0|X) & \text{if}\ \ y=0
\end{array}\right.
\end{equation}
\subsection{Dataset Size}
\end{document}